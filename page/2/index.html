<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"gmrccc.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="GM兔的博客">
<meta property="og:type" content="website">
<meta property="og:title" content="GMR&#39;s Blog">
<meta property="og:url" content="http://gmrccc.com/page/2/index.html">
<meta property="og:site_name" content="GMR&#39;s Blog">
<meta property="og:description" content="GM兔的博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="GMRCCC">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://gmrccc.com/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>GMR's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">GMR's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="GMRCCC"
      src="/uploads/1.jpg">
  <p class="site-author-name" itemprop="name">GMRCCC</p>
  <div class="site-description" itemprop="description">GM兔的博客</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">158</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/GMRCCC" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;GMRCCC" rel="noopener me" target="_blank">GitHub</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/08%20torchvision%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/08%20torchvision%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">08 torchvision中的数据集使用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-30 00:00:00" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-11-01 10:15:49" itemprop="dateModified" datetime="2023-11-01T10:15:49+08:00">2023-11-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>打开PyTorch官网，官方文档中第一部分是PyTorch的核心模块，torchaudio是处理PyTorch语音的，torchtext是处理文本的，torchivision是处理图像的。</p>
<p><img src="/img/Pastedimage20230724200142.png" alt="图片"></p>
<p>打开torchvision，tensorboard和transforms均来源于这里，torchvision分了好几个模块，包括Datasets即数据集的API文档，只要在写代码时指定相应数据集的参数，它就能去下载使用对应的数据集。</p>
<p><img src="/img/Pastedimage20230724200856.png" alt="图片"></p>
<p>COCO数据集一般用于目标检查、语义分割；MINIST一般作为教科书中的入门数据集，是手写文字数据集；CIFAR一般用于物体识别。</p>
<p>torchvision.models中提供了最常用的一些神经网络模块，这些模块已经训练好了。其中有分类、语义分割、目标检测、视频分类等数据集。</p>
<p>torchvision中的tensorboard和transforms模块已经讲解过了。</p>
<h3 id="CIFAR-10-Dataset"><a href="#CIFAR-10-Dataset" class="headerlink" title="CIFAR 10 Dataset"></a>CIFAR 10 Dataset</h3><ul>
<li><p><strong>root</strong> (<em>string</em>) – Root directory of dataset where directory <code>cifar-10-batches-py</code> exists or will be saved to if download is set to True.</p>
</li>
<li><p><strong>train</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>,</em> <em>optional</em>) – If True, creates dataset from training set, otherwise creates from test set.</p>
</li>
<li><p><strong>transform</strong> (<em>callable</em><em>,</em> <em>optional</em>) – A function&#x2F;transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></p>
</li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>,</em> <em>optional</em>) – A function&#x2F;transform that takes in the target and transforms it.</p>
</li>
<li><p><strong>download</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>,</em> <em>optional</em>) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</p>
</li>
</ul>
<ol>
<li>root表示数据集存在什么样的位置。</li>
<li>train是bool变量，为true表示创建的是训练集，为false表示是测试集。</li>
<li>transform表示想对数据集进行什么样的变换。</li>
<li>target_transform是对target进行transform。</li>
<li>download为true表示从网上自动下载数据集，为false则不会下载。</li>
</ol>
<p>首先导入torchvision</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvison</span><br></pre></td></tr></table></figure>

<p>调用datasets工具包，对CIFAR10数据集进行下载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>我们可以对下载得到的数据集进行分析</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="number">0x1C58D023F40</span>&gt;, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>第一部分为PIL的图片数据，第二部分的3代表一个target(类别)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(test_set.classes)</span><br><span class="line"></span><br><span class="line">输出：[<span class="string">&#x27;airplane&#x27;</span>, <span class="string">&#x27;automobile&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>说明3代表的是猫这个类别。</p>
<p>知道了这些后，我们就可以用两个变量来获取test_set的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="number">0x1C583EE91C0</span>&gt;</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>

<p>CIFAR10包含了60000张32×32分为10个类别的彩色图片，其中50000张是训练图像，10000张是测试图像。</p>
<h3 id="利用Transforms进行类型转换"><a href="#利用Transforms进行类型转换" class="headerlink" title="利用Transforms进行类型转换"></a>利用Transforms进行类型转换</h3><p>先实例化transform对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset_transform = torchvision.transform.Compose([torchvision.transforms.ToTensor()])</span><br></pre></td></tr></table></figure>

<p>然后可以在数据集的参数中加入transform参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">True</span>, transform=dataset_transform, download=<span class="literal">True</span>)  </span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=dataset_transform, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>输出图片</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">输出：tensor数据类型</span><br></pre></td></tr></table></figure>

<h3 id="和Tensorboard结合"><a href="#和Tensorboard结合" class="headerlink" title="和Tensorboard结合"></a>和Tensorboard结合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&quot;p10&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">	img, target = test_set[i]</span><br><span class="line">	writer.add_image(<span class="string">&quot;test_set&quot;</span>, img, i)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>打开tensorboard，可以发现生成了step1-10的图片。</p>
<p><img src="/img/Pastedimage20230724205642.png" alt="图片"></p>
<p>其他数据集的参数也类似，可以通过官方文档查看数据集参数，但有些数据集下载的很慢，我们可以先设置download为True，然后打开下载地址用迅雷(或其他)下载速度快的软件下载，下载完成后保存到对应文件夹中，这时再运行代码，它就会调用已经下载好的压缩包进行解压，减少下载所需时间。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/09%20DataLoader%E7%9A%84%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/09%20DataLoader%E7%9A%84%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">09 DataLoader的使用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-30 00:00:00" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-11-01 10:15:52" itemprop="dateModified" datetime="2023-11-01T10:15:52+08:00">2023-11-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>dataset只是去告诉程序我们的数据集在哪个位置，dataloader是一个加载器，可以把数据加载到神经网络中。可以类比成扑克牌，把手想象成一个神经网络。</p>
<p><img src="/img/Pastedimage20230724214614.png" alt="图片"></p>
<p> dataloader每次从dataset当中取数据，取多少、怎么取用dataloader中的参数去控制。</p>
<p>打开Pytorch的官方文档中的dataloader部分。</p>
<p><img src="/img/Pastedimage20230724215614.png" alt="图片"></p>
<p>可以看到，dataloader出现在torch.utils.data工具包下。</p>
<p>dataloader中的参数比较多，但其中只有dataset没有默认值。</p>
<ul>
<li><p><strong>dataset</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) – dataset from which to load the data.</p>
</li>
<li><p><strong>batch_size</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>,</em> <em>optional</em>) – how many samples per batch to load (default: <code>1</code>).</p>
</li>
<li><p><strong>shuffle</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>,</em> <em>optional</em>) – set to <code>True</code> to have the data reshuffled at every epoch (default: <code>False</code>).</p>
</li>
<li><p><strong>sampler</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><em>Sampler</em></a> <em>or</em> <em>Iterable</em><em>,</em> <em>optional</em>) – defines the strategy to draw samples from the dataset. Can be any <code>Iterable</code> with <code>__len__</code> implemented. If specified, <code>shuffle</code> must not be specified.</p>
</li>
<li><p><strong>batch_sampler</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><em>Sampler</em></a> <em>or</em> <em>Iterable</em><em>,</em> <em>optional</em>) – like <code>sampler</code>, but returns a batch of indices at a time. Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code>, and <code>drop_last</code>.</p>
</li>
<li><p><strong>num_workers</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>,</em> <em>optional</em>) – how many subprocesses to use for data loading. <code>0</code> means that the data will be loaded in the main process. (default: <code>0</code>)</p>
</li>
<li><p><strong>drop_last</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>,</em> <em>optional</em>) – set to <code>True</code> to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If <code>False</code> and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: <code>False</code>)</p>
</li>
</ul>
<p>batch_size表示每次取几个数据。</p>
<p>shuffle表示是否打乱数据，若为true，则两次筛数据的顺序一样 。</p>
<p>num_workers表示进程数，若为0则只用主进程进行加载，但<strong>num_workers＞0在windows下可能会产生错误</strong>。</p>
<p>drop_last表示当取数据除不尽数据总数时数据是舍去还是不舍去。</p>
<p>先进行数据的准备，把batch_size设为4，表示一次性抓取4个数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 准备的测试集  </span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor());  </span><br><span class="line">  </span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>查看CIFAR10数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>) -&gt; <span class="type">Tuple</span>[<span class="type">Any</span>, <span class="type">Any</span>]:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;  </span></span><br><span class="line"><span class="string">    Args:        index (int): Index  </span></span><br><span class="line"><span class="string">    Returns:        tuple: (image, target) where target is index of the target class.    &quot;&quot;&quot;</span>    </span><br><span class="line">    img, target = self.data[index], self.targets[index]  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># doing this so that it is consistent with all other datasets  </span></span><br><span class="line">    <span class="comment"># to return a PIL Image    img = Image.fromarray(img)  </span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  </span><br><span class="line">        img = self.transform(img)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  </span><br><span class="line">        target = self.target_transform(target)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> img, target</span><br></pre></td></tr></table></figure>

<p>可以发现getitem返回数据的方式为img, target，则我们用同样的方式去接收。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试数据集中第一张图片及target  </span></span><br><span class="line">img, target = test_data[<span class="number">0</span>]  </span><br><span class="line"><span class="built_in">print</span>(img.shape)  </span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>

<p>batch_size&#x3D;4，就是取test_data[0]，test_data[1]，test_data[2]，test_data[3]并将他们打包。注意，默认情况下batch_size是从数据集中随机抓取数据。</p>
<p><img src="/img/Pastedimage20230724223623.png" alt="图片"></p>
<p><img src="/img/Pastedimage20230724222522.png" alt="图片"></p>
<p>可以看到，dataloader打包时是将img和target分别打包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:  </span><br><span class="line">    imgs, targets = data  </span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)  </span><br><span class="line">    <span class="built_in">print</span>(targets)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">1</span>])</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">tensor([<span class="number">9</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">1</span>])</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>[4,3,32,32]表示有4张图片，3通道，图片大小32×32.<br>[1,6,7,1]表示这4张图片的类别。</p>
<p>用tensorboard演示一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)  </span><br><span class="line">step = <span class="number">0</span>  </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:  </span><br><span class="line">    imgs, targets = data  </span><br><span class="line">    <span class="comment"># print(imgs.shape)  </span></span><br><span class="line">    <span class="comment"># print(targets)    </span></span><br><span class="line">    writer.add_images(<span class="string">&quot;test_data&quot;</span>, imgs, step)  </span><br><span class="line">    step = step + <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>进入tensorboard查看，这里要是跳步的话，可以用如下命令</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --samples_per_plugin images=<span class="number">500</span> --logdir=<span class="string">&quot;dataloader&quot;</span></span><br></pre></td></tr></table></figure>

<p><img src="/img/Pastedimage20230724225721.png" alt="图片"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/10%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%AA%A8%E6%9E%B6-nn.Module%E7%9A%84%E4%BD%BF%E7%94%A8%E3%80%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/10%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%AA%A8%E6%9E%B6-nn.Module%E7%9A%84%E4%BD%BF%E7%94%A8%E3%80%82/" class="post-title-link" itemprop="url">10 神经网络的基本骨架-nn.Module的使用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-30 00:00:00" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-11-01 10:15:57" itemprop="dateModified" datetime="2023-11-01T10:15:57+08:00">2023-11-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>打开Pytorch官网，关于神经网络的包都在torch.nn中</p>
<p><img src="/img/Pastedimage20230725095211.png" alt="图片"></p>
<p>其中NN是Neural Network(神经网络)的缩写。</p>
<p>Containers是神经网络的骨架，它下面的都是需要往骨架中填充的东西，比如Convolution Layers(卷积层)、Pooling layers(池化层)、 Padding Layers(填充层)、Non-linear Activations(非线性激活)、 Normalization Layers(正则化层)。</p>
<p><img src="/img/Pastedimage20230725100634.png" alt="图片"></p>
<p>Containers中有6个模块，最常用的是Module模块，Modele模块是<strong>所有</strong>神经网络的一个基本骨架(类)。</p>
<p><img src="/img/Pastedimage20230725101200.png" alt="图片"></p>
<p>所有的神经网络均继承自MODULE类，MODULE类是神经网络的模版。就像买玩具赛车，MODULE是从商店买回来的原版赛车，而自己写神经网络相当于在这辆赛车上进行改装。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure>

<p>Model是继承自Module的神经网络，重写了init和forward函数，在init方法中，我们必须用super去继承父类的init，然后后面两句是自己额外加的，forward函数即前向传播函数，数据输入神经网络后要经过forward步骤后输出(还有反向传播函数)。</p>
<p><img src="/img/Pastedimage20230725102440.png" alt="图片"></p>
<p>forward参数中的x即我们要输入的东西，我们对x进行了卷积(conv)、非线性处理(relu)，然后在进行一次卷积、非线性处理后返回。</p>
<p>我们看一下forward的官方定义</p>
<p>forward(_*input_)<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward"></a></p>
<p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>forward定义一个计算，应该在所有子类中进行一个重写。</p>
<p>我们可以尝试写一下神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  </span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span>  </span><br><span class="line">        <span class="keyword">return</span> output  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">  </span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)  </span><br><span class="line">output = tudui(x)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">输出：tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure>

<p>在生成类中函数时，我们可以用pycharm自带的Code-&gt;generate来生成</p>
<p><img src="/img/Pastedimage20230725104239.png" alt="图片"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/11%20%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/11%20%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C/" class="post-title-link" itemprop="url">11 卷积操作</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-10-30 00:00:00 / 修改时间：16:46:28" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><img src="/img/Pastedimage20230725105301.png" alt="图片"></p>
<p>1d代表1维，2d代表2维，主要介绍Conv2d。</p>
<h3 id="torch-nn和torch-nn-functional的区别"><a href="#torch-nn和torch-nn-functional的区别" class="headerlink" title="torch.nn和torch.nn.functional的区别"></a>torch.nn和torch.nn.functional的区别</h3><p>torch.nn是对torch.nn.functional的封装，让其更利于我们的使用，可以看作是包含的关系，torch.nn中的内容和torch.nn.functional是一一对应的关系。</p>
<p>torch.nn.functional可以看作是车辆上的一个齿轮的运转，而torch.nn则是把齿轮封装好了提供给我们一个方向盘。</p>
<p>如果我们想要更细致的了解卷积的操作，我们就需要去了解torch.nn.functional。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.functional.conv2d(_input_, _weight_, _bias=None_, _stride=1_, _padding=0_, _dilation=1_, _groups=1_) → [Tensor]</span><br></pre></td></tr></table></figure>

<p><img src="/img/Pastedimage20230725131719.png" alt="图片"></p>
<p>input是一个输入，weight是一个权重即卷积核，bias是偏置，stride是步径，最后是padding。</p>
<p>stride需要多注意一下，它可以是单个数，表示卷积时每次走步数的最小单位，也可以是一个元组，表示向右和向下步数的最小单位。</p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p><img src="/img/Pastedimage20230725141110.png" alt="图片"></p>
<p>我们用python验证一下上面的卷积结果，先输入input和卷积核</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],  </span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])  </span><br><span class="line">  </span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],  </span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  </span><br><span class="line"><span class="built_in">print</span>(kernel.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>可以看到，此时input和kernel的shape只有两个元素，但官方文档中的input和kernel需要四个元素，分别是(N,C,H,W)，N为输入图片的数量，这里只输入了一张，C就是通道数，因为是二维张量，所以也为1。</p>
<ul>
<li>二维张量通常用于表示灰度图像或某些特定类型的数据，其中每个元素对应图像的一个像素或数据的一个单一值。在灰度图像中，每个像素只有一个灰度值，因此二维张量表示的图像只有一个通道。</li>
</ul>
<p>我们用torch自带的reshape函数来规范input和kernel的shape</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))  </span><br><span class="line">kernel = torch.reshape(kernel, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  </span><br><span class="line"><span class="built_in">print</span>(kernel.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>这样就符合了官方文档的规范，可以作为conv的输入了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>)  </span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[<span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>],</span><br><span class="line">          [<span class="number">18</span>, <span class="number">16</span>, <span class="number">16</span>],</span><br><span class="line">          [<span class="number">13</span>,  <span class="number">9</span>,  <span class="number">3</span>]]]])</span><br></pre></td></tr></table></figure>

<p>和图片中的卷积结果一样。</p>
<p>我们将步长改为2再试试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output2 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">2</span>)  </span><br><span class="line"><span class="built_in">print</span>(output2)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[<span class="number">10</span>, <span class="number">12</span>],</span><br><span class="line">          [<span class="number">13</span>,  <span class="number">3</span>]]]])</span><br></pre></td></tr></table></figure>

<p>可以看到变成了一个2×2的矩阵，说明当步长设置为2时，不仅每次向右走2步，向下也是走2步。</p>
<p>padding就是对图片的边缘进行一个填充，可以用一个数或者一个元组来规定扩宽的尺度，比如当padding&#x3D;1时如下图所示</p>
<p><img src="/img/Pastedimage20230725142043.png" alt="图片"></p>
<p>填充值默认为0.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">output3 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  </span><br><span class="line"><span class="built_in">print</span>(output3)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">4</span>, <span class="number">10</span>,  <span class="number">8</span>],</span><br><span class="line">          [ <span class="number">5</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>,  <span class="number">6</span>],</span><br><span class="line">          [ <span class="number">7</span>, <span class="number">18</span>, <span class="number">16</span>, <span class="number">16</span>,  <span class="number">8</span>],</span><br><span class="line">          [<span class="number">11</span>, <span class="number">13</span>,  <span class="number">9</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">          [<span class="number">14</span>, <span class="number">13</span>,  <span class="number">9</span>,  <span class="number">7</span>,  <span class="number">4</span>]]]])</span><br></pre></td></tr></table></figure>

<p>可以看到当stride&#x3D;1，padding&#x3D;1时，输出矩阵和输入矩阵的大小一样。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/12%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8D%B7%E7%A7%AF%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/12%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8D%B7%E7%A7%AF%E5%B1%82/" class="post-title-link" itemprop="url">12 神经网络-卷积层</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-10-30 00:00:00 / 修改时间：17:00:52" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(_in_channels_, _out_channels_, _kernel_size_, _stride=1_, _padding=0_, _dilation=1_, _groups=1_, _bias=True_, _padding_mode=<span class="string">&#x27;zeros&#x27;</span>_, _device=None_, _dtype=None_)</span><br></pre></td></tr></table></figure>

<p>in_channels 输入图像的通道数；out_channels 输出图像的通道数；kernel_size 卷积核的大小；stride 步径的大小；padding 填充；dilation 卷积核当中每一个卷积核对应位的距离；groups 一般为1很少改动，改动的话就是分组卷积；bias 对卷积后的结果是否加减一个常数，常年设置为true；padding_mode padding不为0时使用什么样的模式进行填充。</p>
<p>可以看出只有in_channels，out_channels和kernel_size需要进行设置，日常使用中设置前面5个参数即可。</p>
<p>dilation又叫做空洞卷积，会导致卷积核各个元素间的距离变大，如下图</p>
<p><img src="/img/Pastedimage20230725162230.png" alt="图片"></p>
<p>卷积公式</p>
<p><img src="/img/Pastedimage20230725144718.png" alt="图片"></p>
<p>对于卷积核，我们只需要设置其大小尺寸(kernel_size)，而不用设置其中的数，参数会在卷积过程中不断的调整。</p>
<p>in_channels和out_channels的设置很重要，比如当in_channels&#x3D;1，out_channels&#x3D;1时，模型就会拿一个卷积核进行卷积</p>
<p><img src="/img/Pastedimage20230725150404.png" alt="图片"></p>
<p>若设置out_channels&#x3D;2，则模型会拿两个卷积核进行卷积，把两个结果进行输出</p>
<p><img src="/img/Pastedimage20230725150445.png" alt="图片"></p>
<p>为了更深入了解卷积，我们来看一段代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span>  torchvision  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line">  </span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;p10&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()  </span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        x = self.conv1(x)  </span><br><span class="line">        <span class="keyword">return</span> x  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line"><span class="built_in">print</span>(tudui)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Tudui(</span><br><span class="line">  (conv1): Conv2d(<span class="number">3</span>, <span class="number">6</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们定义了一个名叫Tudui的神经网络，在其中定义了一层卷积层，接下来我们把图像放进神经网络中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">	imgs, targets = data</span><br><span class="line">	output = tudui(imgs)</span><br><span class="line">	<span class="built_in">print</span>(imgs.shape)</span><br><span class="line">	<span class="built_in">print</span>(output.shape)</span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">torch.Size([<span class="number">64</span>, <span class="number">6</span>, <span class="number">30</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure>

<p>可以看到输入的通道数为3，输出的通道数为6</p>
<p>我们可以用tensorboard做一个展示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&quot;../logs&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">step = <span class="number">0</span>  </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:  </span><br><span class="line">    imgs, target = data  </span><br><span class="line">    output = tudui(imgs)  </span><br><span class="line">    <span class="comment"># torch.Size([64, 3, 32, 32])  </span></span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)  </span><br><span class="line">    <span class="comment"># torch.Size([64, 6, 30, 30])  </span></span><br><span class="line">    writer.add_images((<span class="string">&quot;output&quot;</span>, output, step))  </span><br><span class="line">  </span><br><span class="line">    step += <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">报错</span><br></pre></td></tr></table></figure>

<p>报错的原因是输出的图像是3个通道时展示彩色图像，而我们的output有6个通道，无法展示。</p>
<p>我们用reshape将通道数变为3，相应的，batchsize会增加(这样做不严谨，但这里仅仅为了演示)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output = torch.reshape(output, (-<span class="number">1</span>, <span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>))  //-<span class="number">1</span>表示会根据后面的数自动调整batchsize的大小</span><br><span class="line">writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)</span><br></pre></td></tr></table></figure>

<p>输出如下</p>
<p><img src="/img/Pastedimage20230725155804.png" alt="图片"></p>
<h3 id="常用卷积层"><a href="#常用卷积层" class="headerlink" title="常用卷积层"></a>常用卷积层</h3><p>VGG16<br><img src="/img/Pastedimage20230725155919.png" alt="图片"></p>
<p>输出图像的长宽我们可以通过如下公式计算</p>
<p><img src="/img/Pastedimage20230725160942.png" alt="图片"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/13%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9C%80%E5%A4%A7%E5%8C%96%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/13%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9C%80%E5%A4%A7%E5%8C%96%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">13 神经网络-最大化池的使用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-10-30 00:00:00 / 修改时间：17:04:53" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><img src="/img/Pastedimage20230725161216.png" alt="图片"></p>
<p>MaxPool即最大池化或者下采样，MaxUnpool即上采样，还有平均池化等等，但最常用的是MaxPool2d</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxPool2d(_kernel_size_, _stride=None_, _padding=0_, _dilation=1_, _return_indices=False_, _ceil_mode=False_)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>kernel_size</strong>  – the size of the window to take a max over</p>
</li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code>kernel_size</code></p>
</li>
<li><p><strong>padding</strong> – Implicit negative infinity padding to be added on both sides</p>
</li>
<li><p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p>
</li>
<li><p><strong>return_indices</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – if <code>True</code>, will return the max indices along with the outputs. Useful for <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code>torch.nn.MaxUnpool2d</code></a> later</p>
</li>
<li><p><strong>ceil_mode</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – when True, will use ceil instead of floor to compute the output shape</p>
</li>
</ul>
<p>kernel_size 用来取最大值的窗口，比如为3则生成一个3×3的一个窗口<br>stride 和卷积层一样，横向和纵向的大小，但注意这里的默认值是kernel_size<br>padding 和卷积层中一样<br>dilation 和卷积层中一样<br>return_indices 不作了解<br>ceil_mode 为true会使用ceil模式而不是floor模式</p>
<h4 id="Ceil和Floor"><a href="#Ceil和Floor" class="headerlink" title="Ceil和Floor"></a>Ceil和Floor</h4><p><strong>Ceil</strong>： 在池化层中，ceil（天花板函数）指的是向上取整的舍入方式。当进行池化操作时，如果输出尺寸不能整除池化窗口的步幅，ceil会将输出尺寸向上取整到最接近的整数，以确保池化窗口覆盖输入特征图的所有区域。这样做可能会导致输出尺寸稍微大于 floor（向下取整）时的值。</p>
<p><strong>Floor</strong>： 在池化层中，floor（地板函数）指的是向下取整的舍入方式。当进行池化操作时，如果输出尺寸不能整除池化窗口的步幅，floor会将输出尺寸向下取整到最接近的整数，以确保池化窗口在输入特征图上的滑动不会超过输入的边界。这样做可能会导致输出尺寸稍微小于 ceil（向上取整）时的值。</p>
<p>最大池化就是取池化核内的最大值，如下图</p>
<p><img src="/img/Pastedimage20230725163905.png" alt="图片"></p>
<p>默认情况下ceil_mode为false，一般情况下我们只需要设置kernel_size即可</p>
<p>尝试使用一下最大池化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],  </span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],  </span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  </span><br><span class="line">        output = self.maxpool1(<span class="built_in">input</span>)  </span><br><span class="line">        <span class="keyword">return</span> output  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">output = tudui(<span class="built_in">input</span>)  </span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">报错，<span class="string">&quot;max_pool2d&quot;</span> <span class="keyword">not</span> implemented <span class="keyword">for</span> <span class="string">&#x27;Long&#x27;</span></span><br></pre></td></tr></table></figure>

<p>首先我们创建了一个input矩阵，然后将其shape改为4元素以符合神经网络的输入格式，然后我们创建了一个神经网络，使用了kernel_size&#x3D;3向上取整的最大池化层。</p>
<p>我们输入创建的input矩阵，发现报错，这是因为最大池无法对long型进行处理，可以在矩阵的最后面加上类型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">dtype=torch.float32</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">tensor([[[[<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">          [<span class="number">5.</span>, <span class="number">1.</span>]]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到输出了2×2的最大池化后结果。</p>
<h3 id="最大池化的作用"><a href="#最大池化的作用" class="headerlink" title="最大池化的作用"></a>最大池化的作用</h3><p>最大池化的目的是保留图像特征的同时减少数据量，就像在网上看视频，将1080p的输入图像变为720p的输出图像。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/14%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/14%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB/" class="post-title-link" itemprop="url">14 神经网络-非线性激活</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-10-30 00:00:00 / 修改时间：17:11:22" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>前面讲了卷积层和池化层，padding层几乎用不到，比如zeroPad2d，它就是在图像周围用0来进行填充，padding是对输入图像填充的各种方式，但绝大多数在卷积层的padding参数就可以进行设置，所以几乎用不到。</p>
<p><img src="/img/Pastedimage20230725171626.png" alt="图片"></p>
<p>然后我们看一下非线性激活，非线性激活就是给我们的神经网络引入一些非线性的特质，比如说nn.ReLU，当input大于0时取原始值，小于0时截断为0，ReLU的输入必须有batchsize，后面的数据可以自由输入。(新版的好像能自由输入了)</p>
<p><img src="/img/Pastedimage20230725172129.png" alt="图片"></p>
<p>Sigmoid也很常用</p>
<p><img src="/img/Pastedimage20230725172638.png" alt="图片"></p>
<p>ReLU还有一个inplace参数，用来指定是否进行原地修改</p>
<p><img src="/img/Pastedimage20230725173125.png" alt="图片"></p>
<p>一般情况下inplace&#x3D;false，这样可以保留原始数据防止数据丢失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, -<span class="number">0.5</span>],  </span><br><span class="line">                      [-<span class="number">1</span>, <span class="number">3</span>]])  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        self.relu1 = ReLU()  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  </span><br><span class="line">        output = self.relu1(<span class="built_in">input</span>)  </span><br><span class="line">        <span class="keyword">return</span> output  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">  </span><br><span class="line">output = tudui(<span class="built_in">input</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">3.</span>]]]])</span><br></pre></td></tr></table></figure>

<p>可以看到负数都变为了0.</p>
<p>ReLU对图像的处理不是很明显，我们用Sigmoid试一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch  </span><br><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU, Sigmoid  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, -<span class="number">0.5</span>],  </span><br><span class="line">                      [-<span class="number">1</span>, <span class="number">3</span>]])  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))  </span><br><span class="line">  </span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor())  </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        self.relu1 = ReLU()  </span><br><span class="line">        self.sigmoid1 = Sigmoid()  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  </span><br><span class="line">        output = self.sigmoid1(<span class="built_in">input</span>)  </span><br><span class="line">        <span class="keyword">return</span> output  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">  </span><br><span class="line">step = <span class="number">0</span>  </span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)  </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:  </span><br><span class="line">    imgs,targets = data  </span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, global_step=step)  </span><br><span class="line">    output = tudui(imgs)  </span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)  </span><br><span class="line">    step += <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>可以得到如下结果</p>
<p><img src="/img/Pastedimage20230725174337.png" alt="图片"></p>
<p>非线性变换的主要目的是给网络引入各种特征，能够让结果去符合一个各种特征的模型。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/15%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%BA%BF%E6%80%A7%E5%B1%82%E5%8F%8A%E5%85%B6%E4%BB%96%E5%B1%82%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/15%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%BA%BF%E6%80%A7%E5%B1%82%E5%8F%8A%E5%85%B6%E4%BB%96%E5%B1%82%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">15 神经网络-线性层及其他层介绍</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-10-30 00:00:00 / 修改时间：17:23:15" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="Normalization-Layers"><a href="#Normalization-Layers" class="headerlink" title="Normalization Layers"></a>Normalization Layers</h3><p>归一化层就是对我们的输入采用归一化，注意不是正则化。</p>
<h4 id="正则化和归一化的区别"><a href="#正则化和归一化的区别" class="headerlink" title="正则化和归一化的区别"></a>正则化和归一化的区别</h4><p>正则化（Regularization）: 正则化是为了防止机器学习模型过拟合而采取的一种策略，通过在损失函数中引入模型的复杂度惩罚项，使得模型在训练过程中更倾向于简单的解决方案，避免对训练数据过度拟合。常见的有L1正则、L2正则等。</p>
<p>归一化（Normalization）: 归一化是指将数据缩放到一定的范围，以确保不同特征或不同样本之间的数值差异不会对模型训练造成影响。归一化的目的是消除特征之间的量纲差异，使得模型能更快地收敛，同时有助于避免某些特征对模型训练的主导作用。</p>
<p><img src="/img/Pastedimage20230726085741.png" alt="图片"></p>
<p><img src="/img/Pastedimage20230726090527.png" alt="图片"></p>
<p>参数需要注意num_features，它等于channel数。</p>
<h3 id="Recurrent-Layers"><a href="#Recurrent-Layers" class="headerlink" title="Recurrent Layers"></a>Recurrent Layers</h3><p>Recurrent层一般用于文字识别中(NLP)。</p>
<h3 id="Transformer-Layers"><a href="#Transformer-Layers" class="headerlink" title="Transformer Layers"></a>Transformer Layers</h3><p>一般用不到</p>
<h3 id="Dropout-Layers"><a href="#Dropout-Layers" class="headerlink" title="Dropout Layers"></a>Dropout Layers</h3><p>随机失活，在训练的过程中随机把某些元素变为0，概率为p。Dropout层可以防止过拟合。</p>
<h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>嵌入层，用于存NLP中token向量。</p>
<h3 id="Linear-Layers"><a href="#Linear-Layers" class="headerlink" title="Linear Layers"></a>Linear Layers</h3><p><img src="/img/Pastedimage20230726090938.png" alt="图片"></p>
<p>线性层类似这样，输入值在经过一系列线性变换后变为输出值</p>
<p><img src="/img/Pastedimage20230727144257.png" alt="图片"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(_in_features_, _out_features_, _bias=True_, _device=None_, _dtype=None_)</span><br></pre></td></tr></table></figure>

<p>Linear的参数非常的简单，只需指定in_features、out_features和是否需要偏置即可。</p>
<p><img src="/img/Pastedimage20230727151731.png" alt="图片"></p>
<p>其中weight等于k，bias等于b。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line">  </span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>, drop_last=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        self.linear1 = Linear(<span class="number">196608</span>, <span class="number">10</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  </span><br><span class="line">        output = self.linear1(<span class="built_in">input</span>)  </span><br><span class="line">        <span class="keyword">return</span> output  </span><br><span class="line">  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:  </span><br><span class="line">    imgs, targets = data  </span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)  </span><br><span class="line">    output = torch.reshape(imgs, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>))  </span><br><span class="line">    <span class="built_in">print</span>(output.shape)  </span><br><span class="line">    output = tudui(output)  </span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">196608</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<p>64张3通道32×32的图片，通过reshape函数降维成一维向量，再通过神经网络的线性变化把这196608个特征值线性变换为10个特征值。</p>
<p>torch中专门有一个flatten函数将数据降维成线性向量，我们可以用这个函数替换reshape</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output = torch.flatten(imgs);</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">torch.Size([<span class="number">196608</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<p>flatten输出的就是单个数而不是四维矩阵了。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/18%20%E4%BC%98%E5%8C%96%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/18%20%E4%BC%98%E5%8C%96%E5%99%A8/" class="post-title-link" itemprop="url">18 优化器</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-30 00:00:00" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-10-31 15:24:41" itemprop="dateModified" datetime="2023-10-31T15:24:41+08:00">2023-10-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>首先我们需要构造一个优化器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Example:</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer = optim.Adam([var1, var2], lr=<span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure>

<p>SGD、Adam是对应的优化器算法，，然后要放入模型参数和学习速率，另外还有一些不同算法自己需要的参数(比如momentum)</p>
<p>然后是Per-parameter options，这里先不讲。</p>
<p>然后就可以调用这个优化器了，注意使用之前一定要把模型的梯度清零，以防上一步的梯度造成影响。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Example:</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.Adadelta(_params_, _lr=<span class="number">1.0</span>_, _rho=<span class="number">0.9</span>_, _eps=1e-06_, _weight_decay=0_, _foreach=None_, _*_, _maximize=False_, _differentiable=False_)</span><br><span class="line"></span><br><span class="line">torch.optim.Adam(_params_, _lr=<span class="number">0.001</span>_, _betas=(<span class="number">0.9</span>, <span class="number">0.999</span>)_, _eps=1e-08_, _weight_decay=0_, _amsgrad=False_, _*_, _foreach=None_, _maximize=False_, _capturable=False_, _differentiable=False_, _fused=None_)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>可以看到，不同算法优化器之间除了前两个是一样的，其他参数都是有差距的，在入门阶段我们要关注设置parame和lr即可，后面的我们可以采用默认的参数，或者采用别人的参数。</p>
<p>接下来就是使用优化器了，还是继承上一节的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  </span><br><span class="line">  </span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)  </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">1</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        self.model1 = nn.Sequential(  </span><br><span class="line">            nn.Conv2d(<span class="number">3</span> ,<span class="number">32</span> ,<span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),  </span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),  </span><br><span class="line">            nn.Flatten(),  </span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),  </span><br><span class="line">            nn.Linear(<span class="number">64</span> ,<span class="number">10</span>)  </span><br><span class="line">        )  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        x = self.model1(x)  </span><br><span class="line">        <span class="keyword">return</span> x  </span><br><span class="line">  </span><br><span class="line">loss = nn.CrossEntropyLoss()  </span><br><span class="line">tudui = Tudui()  </span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(), lr=<span class="number">0.01</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:  </span><br><span class="line">    imgs, targets = data  </span><br><span class="line">    outputs = tudui(imgs)  </span><br><span class="line">    result_loss = loss(outputs, targets)  </span><br><span class="line">    optim.zero_grad()  </span><br><span class="line">    result_loss.backward()  </span><br><span class="line">    optim.step()</span><br></pre></td></tr></table></figure>

<ol>
<li>在进行反向传播前，我们需要把梯度清零。</li>
<li>在模型刚开始训练时，学习速率lr可以设置的大一些，训练到后面就可以设置的小一些。</li>
<li>优化器本身并不难，难的是优化器中的算法。</li>
</ol>
<p>这样学习只学习了一轮，效果并不显著，所以一般我们会进行多轮学习</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):  </span><br><span class="line">    running_loss = <span class="number">0.0</span>  </span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:  </span><br><span class="line">        imgs, targets = data  </span><br><span class="line">        outputs = tudui(imgs)  </span><br><span class="line">        result_loss = loss(outputs, targets)  </span><br><span class="line">        optim.zero_grad()  </span><br><span class="line">        result_loss.backward()  </span><br><span class="line">        optim.step()  </span><br><span class="line">        running_loss = running_loss + result_loss  </span><br><span class="line">    <span class="built_in">print</span>(running_loss)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">tensor(<span class="number">18750.9902</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">16168.5391</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">15439.5566</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">15950.4600</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">17927.4258</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">20359.9492</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">22133.7930</span>, grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>可以看到随着迭代，损失值刚开始越来越小，但后面却变得越来越大。这是因为学习率设置的过大，梯度下降超出了最优值，导致反向优化。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://gmrccc.com/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/19%20%E7%8E%B0%E6%9C%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E4%BF%AE%E6%94%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/1.jpg">
      <meta itemprop="name" content="GMRCCC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GMR's Blog">
      <meta itemprop="description" content="GM兔的博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | GMR's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/19%20%E7%8E%B0%E6%9C%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E4%BF%AE%E6%94%B9/" class="post-title-link" itemprop="url">19 现有网络模型的使用和修改</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-30 00:00:00" itemprop="dateCreated datePublished" datetime="2023-10-30T00:00:00+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-10-31 15:30:12" itemprop="dateModified" datetime="2023-10-31T15:30:12+08:00">2023-10-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">PyTorch快速入门</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>使用的模型是torchvision中的VGG分类模型，VGG有很多个版本，最常用的是VGG16或VGG19，我们这次用VGG16(注意，这里的网页是老版，版本为0.9.0，可以在左上角设置)。</p>
<p><img src="/img/Pastedimage20230728155829.png" alt="图片"></p>
<ol>
<li>pretrained为true，则下载网络模型的参数在数据集中已经训练好了，false就是初始参数。</li>
<li>progress为true会显示下载进度条，false不显示。</li>
</ol>
<p>我们先下载ImageNet数据集。</p>
<p>下载前必须先安装scipy，有就不用安装，打开termial终端，注意环境</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scipy</span><br></pre></td></tr></table></figure>

<p><img src="/img/Pastedimage20230728161226.png" alt="图片"></p>
<p>根据官网给的参数来下载数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data = torchvision.datasets.ImageNet(<span class="string">&quot;data_image_net&quot;</span>, split=<span class="string">&#x27;train&#x27;</span>, download=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor())</span><br></pre></td></tr></table></figure>

<p>发现报错，去网上搜发现这个数据集有100多个G，如果只是下下来训练一个已知模型的话太过浪费，因此我们不进行下载。</p>
<p>我们下载VGG16预训练和未预训练的两个模型，比较它们之间的差别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line">  </span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)  </span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/img/Pastedimage20230728163045.png" alt="图片"><br><img src="/img/Pastedimage20230728163112.png" alt="图片"></p>
<p>查看VGG16其中一层，前一个是训练好的模型，后一个是未训练的模型，可以看到参数发生了改变。</p>
<p>VGG16模型分类1000类，但我们要训练的数据集可能不是1000类的。一种简单的方式是把out_feature的1000改为10，或者在最后加一层线性层，将输出设为10.</p>
<h4 id="添加"><a href="#添加" class="headerlink" title="添加"></a>添加</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vgg16_true.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="number">1000</span>, <span class="number">10</span>))  </span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    ...</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">	...</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (add_linear): Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到在最后加了一个线性层，我们也可以直接把线性层加到classifier里面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vgg16_true.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="number">1000</span>, <span class="number">10</span>))  </span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    ...</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">	...</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (add_linear): Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vgg16_false.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)  </span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    ...</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    ...</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">GMRCCC</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
